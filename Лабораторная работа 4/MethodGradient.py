import numpy as np

# Определяем функцию
def f(x1, x2):
    return 4 * x1**2 + 5 * x2**2 - 3 * x1 * x2 + 9 * x1 - 2 * x2 + 5

# Градиент функции (вектор частных производных)
def grad_f(x1, x2):
    df_dx1 = 8 * x1 - 3 * x2 + 9  # Частная производная по x1
    df_dx2 = 10 * x2 - 3 * x1 - 2  # Частная производная по x2
    return np.array([df_dx1, df_dx2])

# Градиентный спуск
def gradient_descent(x0, alpha, tol=0.0001, max_iter=1000):
    """
    Градиентный спуск для минимизации функции.

    Параметры:
    - f: оптимизируемая функция
    - grad_f: градиент функции
    - x0: начальные значения переменных (numpy массив)
    - alpha: шаг обучения (скорость спуска)
    - tol: допустимая точность
    - max_iter: максимальное количество итераций

    Возвращает:
    - Минимум функции и соответствующие значения переменных
    """
    x = x0
    for i in range(max_iter):
        grad = grad_f(x[0], x[1])  # Вычисляем градиент
        x_new = x - alpha * grad  # Обновляем значения переменных
        
        # Проверяем условие сходимости
        if abs(f(x_new[0], x_new[1]) - f(x[0], x[1])) < tol:
            #print(f"Сошлось за {i+1} итераций")
            return x_new, f(x_new[0], x_new[1])
        
        x = x_new

    #print("Достигнуто максимальное число итераций")
    return x, f(x[0], x[1])

# Начальные значения переменных
x0 = np.array([2, 3])  # Например, начальная точка (x1, x2) = (2, 3)

# Запуск градиентного спуска
alpha = 0.01  # Шаг обучения
result, f_min = gradient_descent(x0, alpha)




